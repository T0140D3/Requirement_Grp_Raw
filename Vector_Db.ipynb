{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a434992",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"merged_requirements.json\",\"r\") as f:\n",
    "    requirements=json.load(f)\n",
    "req_ids=[]\n",
    "for req in requirements:\n",
    "    req_ids.append(req['req_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf35f6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trace_full_upstream_iterative(req_id,requirements):\n",
    "    \"\"\"\n",
    "    Iteratively trace all upstream requirements based on inputs/outputs\n",
    "    until reaching requirements with no upstream.\n",
    "    \"\"\"\n",
    "    req_index = {req['req_id']: idx for idx, req in enumerate(requirements)}\n",
    "    if req_id not in req_index:\n",
    "        return []\n",
    "\n",
    "    traced = set()           # store all upstream req_ids\n",
    "    to_check = [req_id]      # stack/queue for iterative processing\n",
    "\n",
    "    while to_check:\n",
    "        current_id = to_check.pop()  # take one req_id\n",
    "        current_idx = req_index[current_id]\n",
    "        current_inputs = set(requirements[current_idx]['inputs'])\n",
    "\n",
    "        # Check all previous requirements\n",
    "        for r in requirements[:current_idx]:\n",
    "            r_outputs = set(r['outputs'])\n",
    "            if current_inputs.intersection(r_outputs):\n",
    "                if r['req_id'] not in traced:\n",
    "                    traced.add(r['req_id'])\n",
    "                    to_check.append(r['req_id'])  # add to stack to process its upstream\n",
    "\n",
    "    traced.discard(req_id)  # remove starting requirement if needed\n",
    "    return list(traced)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "828e28a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_downstream_outputs_only(requirements, start_req_id):\n",
    "    \"\"\"\n",
    "    Trace downstream requirements starting from a given req_id.\n",
    "    Downstream = any later requirement that uses the current outputs as inputs.\n",
    "    \"\"\"\n",
    "\n",
    "    # Find starting requirement index and outputs\n",
    "    start_index = None\n",
    "    start_outputs = []\n",
    "    for i, req in enumerate(requirements):\n",
    "        if req[\"req_id\"] == start_req_id:\n",
    "            start_index = i\n",
    "            start_outputs = req.get(\"outputs\", [])\n",
    "            break\n",
    "\n",
    "    if start_index is None:\n",
    "        return []  # req_id not found\n",
    "\n",
    "    downstream = []\n",
    "    to_trace = set(start_outputs)\n",
    "\n",
    "    # Walk forward through requirements\n",
    "    for j in range(start_index + 1, len(requirements)):\n",
    "        req = requirements[j]\n",
    "        inputs = set(req.get(\"inputs\", []))\n",
    "\n",
    "        # If requirement consumes any of the current outputs\n",
    "        if to_trace & inputs:\n",
    "            downstream.append(req[\"req_id\"])\n",
    "            # Expand trace set with this requirement's outputs\n",
    "            to_trace |= set(req.get(\"outputs\", []))\n",
    "\n",
    "    return downstream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29978f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_list(lst):\n",
    "    flat = []\n",
    "    for item in lst:\n",
    "        if isinstance(item, list):\n",
    "            flat.extend(flatten_list(item))\n",
    "        else:\n",
    "            flat.append(item)\n",
    "    return flat\n",
    "def reorder_list(reference, output):\n",
    "    # Keep only items that exist in both, and sort according to reference order\n",
    "    return [req for req in reference if req in output]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0119535f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_upstream_downstream_ech_req(requirements):\n",
    "    req_ids=[]\n",
    "    for req in requirements:\n",
    "        req_ids.append(req['req_id'])\n",
    "    main_list=[]\n",
    "    for id in req_ids:\n",
    "        upstream_ids=[]\n",
    "        upstream_ids=trace_full_upstream_iterative(id,requirements) \n",
    "        # downstream_ids=get_downstream(id,requirements)\n",
    "        downstream_ids=find_downstream_outputs_only(requirements,id)\n",
    "        upstream_ids.append(id)\n",
    "        upstream_ids.append(downstream_ids)\n",
    "        output=flatten_list(upstream_ids)\n",
    "            # print(upstream_ids)\n",
    "        reordered = reorder_list(req_ids, output)\n",
    "        main_list.append({id:reordered})\n",
    "    req_flat_dict = {list(d.keys())[0]: list(d.values())[0] for d in main_list}\n",
    "    return req_flat_dict\n",
    "req_flat_dict=get_upstream_downstream_ech_req(requirements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c463966",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\T0140D3\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector DB created with 23 requirements using context-based embeddings.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# Load requirements JSON file\n",
    "def load_requirements(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Prepare embeddings with content + inputs + outputs\n",
    "def embed_requirements(requirements, model):\n",
    "    texts = []\n",
    "    for req in requirements:\n",
    "        content = req.get(\"Content\", \"\")\n",
    "        inputs = \", \".join(req.get(\"inputs\", []))\n",
    "        outputs = \", \".join(req.get(\"outputs\", []))\n",
    "        actor=\", \".join(req.get(\"actors\", []))\n",
    "        target=\", \".join(req.get(\"targets\", []))\n",
    "        verbs=\", \".join(req.get(\"verbs\", []))\n",
    "        nouns=\", \".join(req.get(\"nouns\", []))\n",
    "        combined_text = f\"Requirement: {content} | Inputs: {inputs} | Outputs: {outputs} | actor:{actor} | target:{target} | verbs:{verbs}| noun:{nouns}\"\n",
    "        texts.append(combined_text)\n",
    "    embeddings = model.encode(texts, convert_to_numpy=True, show_progress_bar=True)\n",
    "    return embeddings\n",
    "\n",
    "# Build Faiss index\n",
    "def build_faiss_index(embeddings):\n",
    "    dim = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dim)  # L2 distance\n",
    "    index.add(embeddings)\n",
    "    return index\n",
    "\n",
    "def save_index(index, path):\n",
    "    faiss.write_index(index, path)\n",
    "\n",
    "def save_mapping(requirements, path):\n",
    "    id_map = [req[\"req_id\"] for req in requirements]\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(id_map, f, indent=2)\n",
    "\n",
    "def main():\n",
    "    requirements = load_requirements(\"merged_requirements.json\")\n",
    "\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "    # Embed using content + inputs + outputs\n",
    "    embeddings = embed_requirements(requirements, model)\n",
    "\n",
    "    index = build_faiss_index(embeddings)\n",
    "\n",
    "    save_index(index, \"requirements.index\")\n",
    "    save_mapping(requirements, \"id_map.json\")\n",
    "\n",
    "    print(f\"Vector DB created with {len(requirements)} requirements using context-based embeddings.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "febcccf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'req_id': 'REQ-0939555 D', 'distance': 0.29784950613975525}, {'req_id': 'REQ-0986424 C', 'distance': 0.518606960773468}]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "def load_index(path=\"requirements.index\"):\n",
    "    return faiss.read_index(path)\n",
    "\n",
    "def load_id_map(path=\"id_map.json\"):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def query_vector_db(query_text, model, index, id_map, top_k=20):\n",
    "    query_vec = model.encode([query_text], convert_to_numpy=True)\n",
    "    distances, indices = index.search(query_vec, top_k)\n",
    "    results = []\n",
    "    for dist, idx in zip(distances[0], indices[0]):\n",
    "        results.append({\"req_id\": id_map[idx], \"distance\": float(dist)})\n",
    "    return results\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "index = load_index(\"requirements.index\")\n",
    "id_map = load_id_map(\"id_map.json\")\n",
    "\n",
    "query = \"IF the theft alarm sound warning is available(AlarmSoundWarningAvailability = True)AND the horn is requested for theft alarm system(TheftAlarmStatus = Active Alarm State Horn)         THEN the warning audible signal shall be controlled at the frequency (HORN_ALARM_FREQ) with a (HORN_ALARM_DUTY_CYCLE) duty cycle.(SoundWarningCtrl = 100%)\"\n",
    "results = query_vector_db(query, model, index, id_map,top_k=2)\n",
    "print(results)# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1aba1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cases=[\"Horn Control\",\"Sound Warning Management\",\"Manual Sound Warning\",\"Theft Alarm Sound Warning Control\",\"Panic Mode Sound Warning Control\",\"Thermal Runaway hazard sound warning Control\",\"FOTA Update Sound Warning inhibition\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c220635e",
   "metadata": {},
   "outputs": [],
   "source": [
    "usecase=[]\n",
    "for use in use_cases:\n",
    "    l=[]\n",
    "    end_end_list=[]\n",
    "    results = query_vector_db(use, model, index, id_map,top_k=2)\n",
    "    l.append(results[0]['req_id'])\n",
    "    # l.append(results[1]['req_id'])\n",
    "    end_end_list.append(req_flat_dict[l[0]])\n",
    "    # end_end_list.append(req_flat_dict[l[1]])\n",
    "    end_end_list=flatten_list(end_end_list)\n",
    "    reordered = reorder_list(req_ids, end_end_list)\n",
    "    usecase_obj = {\n",
    "        \"usecase\": use,\n",
    "        \"req_ids\": reordered,\n",
    "    }\n",
    "    usecase.append(usecase_obj)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34216532",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"requirements.json\",\"r\") as f:\n",
    "    raw_requirments=json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00cd66db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"flow.csv\")\n",
    "df.head(5)\n",
    "flow=df[df['Status']==\"PLM Parameters\"]['flowtitle'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b4622ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_External_internal(var_name):\n",
    "    match = df[df['flowtitle'] == var_name]\n",
    "    \n",
    "    if not match.empty:\n",
    "        if (match['P/C'].iloc[0]==\"P\") or (match['P/C'].iloc[0]==\"C\"):\n",
    "            var = f\"{var_name}[{match['P/C'].iloc[0]}][{match['IJK'].iloc[0]}]\"\n",
    "        else:\n",
    "            var=f\"{var_name}[{match['P/C'].iloc[0]}]\"\n",
    "            # check if any row matched\n",
    "        \n",
    "    else:\n",
    "        var = f\"{var_name}[NA]\"\n",
    "    \n",
    "    return var\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d4b08c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coverage_req_ids(result,raw_requirments):\n",
    "    req=[]\n",
    "    for us in result:\n",
    "        re=us['Req_ids']\n",
    "        for j in re:\n",
    "            req.append(j['req_id'])\n",
    "    Result_req=list(set(req))\n",
    "    raw_id=[]\n",
    "    for i in raw_requirments:\n",
    "        req_id=i['req_id']\n",
    "        parts = req_id.rsplit(\" \", 1)  # split into [\"REQ-34545\", \"A\"]\n",
    "        req_id1 = parts[0] + parts[1]\n",
    "        raw_id.append(req_id1)\n",
    "    raw_id=list(set(raw_id))        \n",
    "    missing_in_list2 = [item for item in raw_id if item not in Result_req]\n",
    "    coverage=len(Result_req)/len(raw_id)*100\n",
    "    if coverage==100.00:\n",
    "        return coverage,\" \"\n",
    "    else:\n",
    "        return coverage,missing_in_list2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "407ef650",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "\n",
    "# Iterate over each use case\n",
    "for t in usecase:\n",
    "    usecase_name=t['usecase']\n",
    "    req_ids=t['req_ids']\n",
    "    usecase_inputs = []\n",
    "    usecase_outputs = []\n",
    "    diversity=[]\n",
    "\n",
    "    # # Collect inputs/outputs from each req_id\n",
    "    for req_id in req_ids:\n",
    "        for req in requirements:\n",
    "            if req['req_id']==req_id:\n",
    "                usecase_inputs.extend(req.get(\"inputs\", []))\n",
    "                # usecase_outputs.extend(req.get(\"outputs\", []))\n",
    "        parts = req_id.rsplit(\" \", 1)  # split into [\"REQ-34545\", \"A\"]\n",
    "        req_id1 = parts[0] + \"  \" + parts[1]\n",
    "        for raw_req in raw_requirments:\n",
    "            if raw_req['req_id']==req_id1:\n",
    "                diversity.append({\"req_id\":req_id,\"Content\":raw_req['content'],\"DiversityExpression\":raw_req['diversi']})\n",
    "    \n",
    "    if req_ids:\n",
    "        last_ouput=req_ids[-1]\n",
    "        for req in requirements:\n",
    "            if req['req_id']==last_ouput:\n",
    "                usecase_outputs.extend(req.get(\"outputs\", []))\n",
    "    else:\n",
    "        usecase_outputs=\"\"\n",
    "            \n",
    "            \n",
    "\n",
    "    # # Remove duplicates\n",
    "    usecase_inputs = list(set(usecase_inputs))\n",
    "    usecase_outputs = list(set(usecase_outputs))\n",
    "    matches1 = list(set(flow) & set(usecase_inputs))\n",
    "    matches2 = list(set(flow) & set(usecase_outputs))\n",
    "    \n",
    "    \n",
    "    inputs=[]\n",
    "    for i in usecase_inputs:\n",
    "        var=get_External_internal(i)\n",
    "        inputs.append(var)\n",
    "    outputs=[]\n",
    "    for j in  usecase_outputs:\n",
    "        var=get_External_internal(j)\n",
    "        outputs.append(var)\n",
    "         \n",
    "    \n",
    "\n",
    "    # # Build usecase object\n",
    "    usecase_obj = {\n",
    "        \"usecase\": usecase_name,\n",
    "        # \"req_ids\": req_ids,\n",
    "        \"inputs\": inputs,\n",
    "        \"outputs\": outputs,\n",
    "        \"PLM Parameters\":matches1+matches2,\n",
    "        \"Caliberation Parameters\":\"\",\n",
    "        \"Req_ids\":diversity\n",
    "    }\n",
    "\n",
    "    result.append(usecase_obj)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e00fad92",
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage,missing_req_ids=get_coverage_req_ids(result,raw_requirments)\n",
    "result.append({\"Coverage\":coverage,\"Ungrouped_reqids\":missing_req_ids})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d2ab8b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"usecases_fina_test1.json\", \"w\") as f:\n",
    "    json.dump(result, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
